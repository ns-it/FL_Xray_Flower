{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ns-it/FL_Xray_Flower/blob/main/Path2_MobileNetV2_Experiment_Sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSk8W8-bqRy3"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow\n",
        "!pip install -q flwr[simulation] matplotlib numpy pandas kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fc72018"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- CONFIG: centralize hyperparameters here ---\n",
        "# NUM_ROUNDS defined in CONFIG cell above\n",
        "NUM_ROUNDS = 15\n",
        "NUM_CLIENTS = 10\n",
        "MIN_CLIENTS = int(NUM_CLIENTS * 0.8)\n",
        "BATCH_SIZE = 32\n",
        "LOCAL_EPOCHS_PER_ROUND = 5         # local epochs per round (E)\n",
        "CLIENT_LR = 0.0005                 # learning rate used on clients (SGD)\n",
        "SERVER_ETA = 0.005                 # server optimizer eta (FedAdam)\n",
        "SERVER_BETA1 = 0.9\n",
        "SERVER_BETA2 = 0.999\n",
        "PROXIMAL_MU = 0.01                 # recommended FedProx mu (try 0.001,0.01,0.1 in sweeps)\n",
        "ALPHA = 0.5  # defined in CONFIG                        # Dirichlet alpha (large => near-IID); change to 0.1/0.5 to create non-IID\n",
        "SEED = 42\n",
        "OPTIMIZER = \"sgd\"\n",
        "MOMENTUM=0.9\n",
        "# set random seeds for reproducibility\n",
        "import numpy as np, random, tensorflow as tf, os\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "# End CONFIG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxIUWMcBinBU"
      },
      "outputs": [],
      "source": [
        "# --- 1. استيراد المكتبات (نسخة محدثة لحل مشكلة CUDA) ---\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import flwr as fl\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import logging\n",
        "\n",
        "# --- Imports for Transfer Learning ---\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Input\n",
        "from tensorflow.keras.models import Model\n",
        "# -------------------------------------\n",
        "\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "#  إضافات جديدة لتعريف الأنواع (Types)\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "from flwr.common import NDArrays, Scalar\n",
        "from typing import Dict, Optional, Tuple\n",
        "\n",
        "# إخفاء رسائل التحذير الطويلة\n",
        "logging.getLogger(\"flwr\").setLevel(logging.ERROR)\n",
        "\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "#  التعديل الجديد (الحل الصحيح)\n",
        "#  بدلاً من إخفاء الـ GPU، سنقوم بضبطه ليسمح \"بنمو الذاكرة\"\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "# 1. إجبار TensorFlow على \"نمو الذاكرة\"\n",
        "# هذا يمنع الجلسة الرئيسية من حجز كل الذاكرة، ويترك الباقي لـ Ray\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # اجعل TensorFlow يخصص الذاكرة عند الحاجة فقط (Politely)\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    print(f\"--- تم تفعيل نمو الذاكرة (Memory Growth) للـ GPU ---\")\n",
        "  except RuntimeError as e:\n",
        "    # يجب أن يتم هذا قبل بدء TensorFlow\n",
        "    print(e)\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"Flower Version: {fl.__version__}\")\n",
        "print(f\"NumPy Version: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xseg1J1Xidm6"
      },
      "outputs": [],
      "source": [
        "# --- 2. إعداد Kaggle وتحميل البيانات ---\n",
        "\n",
        "# ارفع ملف kaggle.json\n",
        "if not os.path.exists(\"/root/.kaggle/kaggle.json\"):\n",
        "    print(\"يرجى رفع ملف kaggle.json\")\n",
        "    uploaded = files.upload()\n",
        "    for fn in uploaded.keys():\n",
        "        !mkdir -p ~/.kaggle\n",
        "        !mv {fn} ~/.kaggle/\n",
        "        !chmod 600 ~/.kaggle/kaggle.json\n",
        "else:\n",
        "    print(\"ملف kaggle.json موجود بالفعل.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aag-NBM7iyec"
      },
      "outputs": [],
      "source": [
        "# --- 3. تحميل وفك ضغط البيانات ---\n",
        "if not os.path.exists(\"chest_xray\"):\n",
        "    print(\"تحميل وفك ضغط مجموعة البيانات...\")\n",
        "    !kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        "    !unzip -q chest-xray-pneumonia.zip\n",
        "    print(\"تم فك ضغط البيانات.\")\n",
        "else:\n",
        "    print(\"مجلد البيانات chest_xray موجود بالفعل.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLnHnZMmi1SZ"
      },
      "outputs": [],
      "source": [
        "# --- 4. تجهيز البيانات (Data Preprocessing) ---\n",
        "# --- (نسخة معدلة لـ MobileNetV2) ---\n",
        "\n",
        "# استيراد الدالة الخاصة بالتهيئة\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "# تحديد مسارات المجلدات\n",
        "base_dir = 'chest_xray'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# مسارات فئات التدريب\n",
        "train_normal_dir = os.path.join(train_dir, 'NORMAL')\n",
        "train_pneumonia_dir = os.path.join(train_dir, 'PNEUMONIA')\n",
        "\n",
        "num_normal = len(os.listdir(train_normal_dir))\n",
        "num_pneumonia = len(os.listdir(train_pneumonia_dir))\n",
        "print(f\"Number of (NORMAL) training Images: {num_normal}\")\n",
        "print(f\"Number of (PNEUMONIA) training Images: {num_pneumonia}\")\n",
        "\n",
        "# تحديد الثوابت\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 32 # (موجودة في خلية الإعدادات أصلاً)\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input, # <--- تعديل هنا\n",
        "    validation_split=0.2,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15,\n",
        "    zoom_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    shear_range=0.1\n",
        ")\n",
        "\n",
        "# مولد بيانات التحقق والاختبار (فقط تهيئة)\n",
        "val_test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "# مولد التدريب (80% من بيانات train_dir)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    color_mode='rgb'\n",
        ")\n",
        "\n",
        "validation_generator = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    validation_split=0.2\n",
        ").flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    color_mode='rgb',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# مولد الاختبار (من test_dir)\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    color_mode='rgb',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E4n-ehVi5Jx"
      },
      "source": [
        "---\n",
        "\n",
        "##  معالجة عدم توازن الفئات (Class Imbalance)\n",
        "\n",
        "نلاحظ أن عدد صور الالتهاب الرئوي (Pneumonia) أكبر بكثير من الصور الطبيعية (Normal). سنقوم بحساب \"أوزان الفئات\" (Class Weights) لإجبار النموذج على إعطاء \"عقوبة\" أكبر عند الخطأ في تصنيف الفئة الأقل (Normal)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QJiiDysi6k7"
      },
      "outputs": [],
      "source": [
        "# --- 5. حساب أوزان الفئات ---\n",
        "class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "print(f\"Class Weights (أوزان الفئات): {class_weight_dict}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hb7GFISi8Nh"
      },
      "source": [
        "---\n",
        "\n",
        "## بناء نموذج CNN\n",
        "\n",
        "سنقوم بتعريف دالة لإنشاء نموذج CNN بسيط."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogr7KsQ0i9He"
      },
      "outputs": [],
      "source": [
        "# --- 6. بناء النموذج (Model Definition) ---\n",
        "# --- (نسخة معدلة لاستخدام MobileNetV2 و Transfer Learning) ---\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "#  التعديل: تعريف دالة بناء النموذج الجديدة\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "def create_cnn_model():\n",
        "    # 1. تحديد مدخلات النموذج (يجب أن تكون 3 قنوات لـ MobileNetV2)\n",
        "    inputs = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "\n",
        "    # 2. تحميل نموذج MobileNetV2\n",
        "    # include_top=False: لإزالة طبقة التصنيف الأصلية (1000 فئة)\n",
        "    # weights='imagenet': لتحميل الأوزان التي تعلمها من ImageNet\n",
        "    base_model = MobileNetV2(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_tensor=inputs\n",
        "    )\n",
        "\n",
        "    # 3. تجميد النموذج الأساسي (أهم خطوة في Transfer Learning)\n",
        "    # لن يتم تحديث أوزان MobileNetV2 أثناء التدريب\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # 4. إضافة طبقات التصنيف الخاصة بنا\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x) # <--- لتقليل الأبعاد\n",
        "    x = Dense(128, activation='relu')(x) # <--- طبقة كثيفة لنتعلم منها\n",
        "    x = Dropout(0.5)(x)                  # <--- لتقليل Overfitting\n",
        "    outputs = Dense(1, activation='sigmoid')(x) # <--- طبقة المخرجات\n",
        "\n",
        "    # 5. بناء النموذج النهائي\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # 6. تحديد المُحسِّن (Optimizer)\n",
        "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    #  هام: عند استخدام Transfer Learning، نبدأ بمعدل تعلم (LR) أقل\n",
        "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "    # سنستخدم 0.001 بدلاً من 0.01 (المعرفة في خلية الإعدادات)\n",
        "    # هذا يمنع تدمير الأوزان الجيدة التي جاءت من ImageNet\n",
        "    transfer_learning_lr = CLIENT_LR\n",
        "\n",
        "    if (OPTIMIZER == \"sgd\"):\n",
        "        optimizer = SGD(learning_rate=transfer_learning_lr, momentum=MOMENTUM, clipnorm=1.0)\n",
        "    elif (OPTIMIZER == \"adam\"):\n",
        "        optimizer = Adam(learning_rate=transfer_learning_lr, clipnorm=1.0)\n",
        "\n",
        "    # 7. تجميع النموذج (Compile)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=BinaryCrossentropy(name='loss'),\n",
        "                  metrics=['accuracy',\n",
        "                           Precision(name='precision'),\n",
        "                           Recall(name='recall'),\n",
        "                           AUC(name='auc')])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pnoIGr_i_iR"
      },
      "source": [
        "---\n",
        "\n",
        "##  التدريب المركزي (Centralized Training)\n",
        "\n",
        "الآن، سنقوم بتدريب النموذج بالطريقة التقليدية (المركزية) باستخدام جميع بيانات التدريب. سنستخدم `class_weight_dict` الذي حسبناه."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_1xuJ-CjAKS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "backup_file = 'centralized_training_backup.pkl'\n",
        "\n",
        "centralized_model = create_cnn_model()\n",
        "centralized_model.summary()\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n--- Starting Centralized Model Training (with Early Stopping) ---\")\n",
        "\n",
        "history = centralized_model.fit(\n",
        "        train_generator,\n",
        "        epochs=50,\n",
        "        validation_data=validation_generator,\n",
        "        class_weight=class_weight_dict,\n",
        "        callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "\n",
        "print(\"\\n--- Evaluating Centralized Model (Best Version) ---\")\n",
        "results = centralized_model.evaluate(test_generator, return_dict=True)\n",
        "loss = results['loss']\n",
        "accuracy = results['accuracy']\n",
        "precision = results.get('precision', 0.0)\n",
        "recall = results.get('recall', 0.0)\n",
        "auc = results.get('auc', 0.0)\n",
        "\n",
        "print(f\"Centralized Learning Loss: {loss:.4f}\")\n",
        "print(f\"Centralized Learning Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Centralized Learning Precision: {precision:.4f}\")\n",
        "print(f\"Centralized Learning Recall: {recall:.4f}\")\n",
        "print(f\"Centralized Learning AUC: {auc:.4f}\")\n",
        "\n",
        "\n",
        "centralized_results = results\n",
        "\n",
        "\n",
        "centralized_history_dict = history.history\n",
        "\n",
        "data_to_save = {\n",
        "        'history_dict': centralized_history_dict,\n",
        "        'results_dict': centralized_results\n",
        "}\n",
        "with open(backup_file, 'wb') as f:\n",
        "        pickle.dump(data_to_save, f)\n",
        "print(f\"--- ✅ تم حفظ نتائج التدريب المركزي في {backup_file} ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RLAGKzljCTb"
      },
      "source": [
        "---\n",
        "\n",
        "## الخطوة 8: تجهيز البيانات للتعلم الاتحادي (FL)\n",
        "\n",
        "لمحاكاة التعلم الاتحادي، نحتاج إلى تحميل البيانات كـ `Numpy arrays` ثم تقسيمها. سنقوم **بخلط (Shuffle)** البيانات قبل التقسيم لضمان أن كل عميل يحصل على مزيج من الفئتين (IID)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--fzq2aPjDJT"
      },
      "outputs": [],
      "source": [
        "# --- 8. إعداد التعلم الاتحادي (FL Setup - محاكاة Non-IID واقعي) ---\n",
        "# --- (نسخة معدلة لـ MobileNetV2) ---\n",
        "\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "# دالة لتحميل البيانات كـ NumPy arrays (تبقى كما هي)\n",
        "def load_data_as_numpy(generator):\n",
        "    images, labels = next(generator)\n",
        "    return images, labels\n",
        "\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "#  التعديل: استخدام دالة التهيئة و 'rgb'\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "# مولد لتحميل كل بيانات التدريب (80% split) مرة واحدة\n",
        "# (يجب أن نستخدم train_generator.n الذي تم حسابه في الخلية 8)\n",
        "train_full_generator = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input, # <--- تعديل هنا\n",
        "    validation_split=0.2\n",
        ").flow_from_directory(\n",
        "    train_dir, target_size=IMG_SIZE,\n",
        "    batch_size=train_generator.n,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    color_mode='rgb', # <--- تعديل هنا\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# مولد لتحميل كل بيانات الاختبار مرة واحدة\n",
        "# (يجب أن نستخدم test_generator.n الذي تم حسابه في الخلية 8)\n",
        "test_full_generator = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input # <--- تعديل هنا\n",
        ").flow_from_directory(\n",
        "    test_dir, target_size=IMG_SIZE,\n",
        "    batch_size=test_generator.n,\n",
        "    class_mode='binary',\n",
        "    color_mode='rgb', # <--- تعديل هنا\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(\"\\nLoading data for FL simulation...\")\n",
        "x_train_fl, y_train_fl = load_data_as_numpy(train_full_generator)\n",
        "x_test_fl, y_test_fl = load_data_as_numpy(test_full_generator)\n",
        "\n",
        "print(f\"FL training data shape: {x_train_fl.shape}\")\n",
        "print(f\"FL training labels shape: {y_train_fl.shape}\")\n",
        "print(f\"FL test data shape: {x_test_fl.shape}\")\n",
        "print(f\"FL test labels shape: {y_test_fl.shape}\")\n",
        "\n",
        "\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "#  الجزء الخاص بتوزيع ديريخليه (Dirichlet)\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "NUM_CLIENTS = 10 # (معرف في خلية الإعدادات)\n",
        "NUM_CLASSES = 2 # (0: NORMAL, 1: PNEUMONIA)\n",
        "ALPHA = ALPHA  # (معرف في خلية الإعدادات)\n",
        "\n",
        "print(f\"\\nPartitioning data into {NUM_CLIENTS} clients using Dirichlet (Alpha={ALPHA})...\")\n",
        "\n",
        "# 1. فصل مؤشرات (indices) كل فئة\n",
        "class_indices = [np.where(y_train_fl == i)[0] for i in range(NUM_CLASSES)]\n",
        "\n",
        "# 2. إنشاء قوائم فارغة لكل عميل\n",
        "client_data_indices = [[] for _ in range(NUM_CLIENTS)]\n",
        "\n",
        "# 3. توزيع مؤشرات كل فئة على حدة\n",
        "for class_idx in range(NUM_CLASSES):\n",
        "    indices_for_class = class_indices[class_idx]\n",
        "    num_samples_in_class = len(indices_for_class)\n",
        "\n",
        "    # خلط المؤشرات داخل الفئة\n",
        "    np.random.shuffle(indices_for_class)\n",
        "\n",
        "    # إنشاء توزيع ديريخليه\n",
        "    proportions = np.random.dirichlet([ALPHA] * NUM_CLIENTS)\n",
        "\n",
        "    # حساب عدد العينات لكل عميل\n",
        "    client_samples_per_class = (proportions * num_samples_in_class).astype(int)\n",
        "\n",
        "    # معالجة البواقي\n",
        "    remainder = num_samples_in_class - client_samples_per_class.sum()\n",
        "    client_samples_per_class[np.argmax(proportions)] += remainder\n",
        "\n",
        "    # 4. تقسيم المؤشرات\n",
        "    start = 0\n",
        "    for client_id in range(NUM_CLIENTS):\n",
        "        num_samples = client_samples_per_class[client_id]\n",
        "        end = start + num_samples\n",
        "        client_data_indices[client_id].extend(indices_for_class[start:end])\n",
        "        start = end\n",
        "\n",
        "# 5. إنشاء بيانات العميل النهائية\n",
        "client_data = []\n",
        "print(\"\\n--- Client Data Distribution (Non-IID) ---\")\n",
        "for client_id in range(NUM_CLIENTS):\n",
        "    client_indices = client_data_indices[client_id]\n",
        "    np.random.shuffle(client_indices)\n",
        "\n",
        "    client_x = x_train_fl[client_indices]\n",
        "    client_y = y_train_fl[client_indices]\n",
        "\n",
        "    client_data.append((client_x, client_y))\n",
        "\n",
        "    if client_id < 10: # طباعة كل العملاء\n",
        "        num_normal = np.sum(client_y == 0)\n",
        "        num_pneumonia = np.sum(client_y == 1)\n",
        "        print(f\"Client {client_id}: Total={len(client_y)} | NORMAL={num_normal} | PNEUMONIA={num_pneumonia}\")\n",
        "\n",
        "print(\"...\")\n",
        "print(f\"Data partitioned on: {len(client_data)} Clients (Realistic Non-IID).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utMhZdUodF-0"
      },
      "outputs": [],
      "source": [
        "# ---  عرض توزيع البيانات (Data Distribution Visualization) ---\n",
        "\n",
        "# 1. تجميع الإحصائيات من 'client_data'\n",
        "# (نفترض أن 0 = NORMAL, 1 = PNEUMONIA)\n",
        "distribution_data = []\n",
        "for client_id in range(NUM_CLIENTS):\n",
        "    client_y = client_data[client_id][1]\n",
        "    num_normal = np.sum(client_y == 0)\n",
        "    num_pneumonia = np.sum(client_y == 1)\n",
        "    distribution_data.append({\n",
        "        \"Client\": f\"C{client_id}\",\n",
        "        \"NORMAL\": num_normal,\n",
        "        \"PNEUMONIA\": num_pneumonia\n",
        "    })\n",
        "\n",
        "# 2. تحويلها إلى DataFrame لسهولة الرسم\n",
        "df_dist = pd.DataFrame(distribution_data)\n",
        "\n",
        "# 3. إعداد بيانات الرسم البياني المكدس (Stacked Bar Chart)\n",
        "client_labels = df_dist[\"Client\"]\n",
        "normal_counts = df_dist[\"NORMAL\"]\n",
        "pneumonia_counts = df_dist[\"PNEUMONIA\"]\n",
        "\n",
        "# 4. إنشاء الرسم البياني\n",
        "plt.figure(figsize=(12, 6))\n",
        "bar_width = 0.7\n",
        "\n",
        "# رسم الفئة الأولى (NORMAL)\n",
        "plt.bar(client_labels, normal_counts, bar_width, label='NORMAL (Class 0)', color='blue')\n",
        "\n",
        "# رسم الفئة الثانية (PNEUMONIA) فوقها\n",
        "plt.bar(client_labels, pneumonia_counts, bar_width, bottom=normal_counts, label='PNEUMONIA (Class 1)', color='orange')\n",
        "\n",
        "plt.title(f'Data Distribution per Client (ALPHA = {ALPHA})')\n",
        "plt.xlabel('Client ID')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# إضافة عدد العينات الكلي فوق كل عمود\n",
        "totals = normal_counts + pneumonia_counts\n",
        "for i, total in enumerate(totals):\n",
        "    plt.text(i, total + 5, str(total), ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH_hWrbLjGSB"
      },
      "source": [
        "---\n",
        " تعريف العميل الاتحادي (FL Client)\n",
        "\n",
        "سنقوم بتعريف `class` يمثل العميل. كل عميل سيقوم بـ:\n",
        "1.  استلام النموذج العام (Global Model).\n",
        "2.  تقسيم بياناته المحلية إلى تدريب وتحقق (80/20).\n",
        "3.  حساب أوزان الفئات (Class Weights) *المحلية* الخاصة به.\n",
        "4.  تطبيق زيادة البيانات (Augmentation) *أثناء* التدريب المحلي.\n",
        "5.  التدريب لـ 3 دورات (Epochs) محلياً.\n",
        "6.  إرسال الأوزان المحدثة."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3mc6qxnjHjZ"
      },
      "outputs": [],
      "source": [
        "# --- 9. تعريف العميل الاتحادي (FL Client Definition) ---\n",
        "# --- (نسخة مُصححة من الحلقة اللانهائية) ---\n",
        "import math # <--- إضافة لاستخدام math.ceil\n",
        "\n",
        "class XRayClient(fl.client.NumPyClient):\n",
        "\n",
        "    def __init__(self, model, x_client, y_client, client_id):\n",
        "        self.model = model\n",
        "        self.client_id = client_id\n",
        "\n",
        "        # مولد بيانات للتدريب المحلي مع زيادة البيانات\n",
        "        self.train_datagen = ImageDataGenerator(\n",
        "            # preprocessing_function=preprocess_input, # <--- تعديل هام جداً\n",
        "            rotation_range=15,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            zoom_range=0.1,\n",
        "            horizontal_flip=True\n",
        "        )\n",
        "\n",
        "        # التحقق من عدد العينات قبل التقسيم (من الإصلاح السابق)\n",
        "        if len(y_client) < 2:\n",
        "            print(f\"[Client {self.client_id}]: Warning: Received only {len(y_client)} samples. Using all for training.\")\n",
        "            self.x_train, self.y_train = x_client, y_client\n",
        "            self.x_val, self.y_val = x_client[0:0], y_client[0:0]\n",
        "        else:\n",
        "            try:\n",
        "                self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(\n",
        "                    x_client, y_client, test_size=0.2, random_state=42, stratify=y_client\n",
        "                )\n",
        "            except ValueError:\n",
        "                print(f\"[Client {self.client_id}]: Warning: Stratify failed. Using regular split.\")\n",
        "                self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(\n",
        "                    x_client, y_client, test_size=0.2, random_state=42\n",
        "                )\n",
        "\n",
        "    @tf.function\n",
        "    def _train_step(self, x_batch, y_batch, global_weights_tensors, mu, sample_weights_tensor):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self.model(x_batch, training=True)\n",
        "            loss = self.model.loss(y_batch, y_pred)\n",
        "\n",
        "            if sample_weights_tensor is not None:\n",
        "                loss = loss * sample_weights_tensor\n",
        "                loss = tf.reduce_mean(loss)\n",
        "\n",
        "            if mu > 0.0:\n",
        "                prox_term = 0.0\n",
        "                for model_weight, global_weight in zip(self.model.trainable_weights, global_weights_tensors):\n",
        "                    prox_term += tf.reduce_sum(tf.square(model_weight - global_weight))\n",
        "                loss += (mu / 2) * prox_term\n",
        "\n",
        "        grads = tape.gradient(loss, self.model.trainable_weights)\n",
        "        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return self.model.get_weights()\n",
        "\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "\n",
        "        print(f\"\\n--- [Client {self.client_id}]: Starting fit() for Round ---\")\n",
        "\n",
        "        self.model.set_weights(parameters)\n",
        "        global_weights_tensors = [tf.convert_to_tensor(w) for w in parameters]\n",
        "        mu = config.get(\"mu\", 0.0)\n",
        "\n",
        "        if mu > 0.0:\n",
        "            print(f\"[Client {self.client_id}]: Mode = FedProx (mu={mu})\")\n",
        "        else:\n",
        "            print(f\"[Client {self.client_id}]: Mode = FedAvg (mu=0.0)\")\n",
        "\n",
        "        # (حساب أوزان الفئات)\n",
        "        local_weight_dict = None\n",
        "        if len(np.unique(self.y_train)) > 1:\n",
        "            try:\n",
        "                local_weights = compute_class_weight('balanced', classes=np.unique(self.y_train), y=self.y_train)\n",
        "                local_weight_dict = dict(enumerate(local_weights))\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "        #  التعديل: حساب steps_per_epoch\n",
        "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "        # BATCH_SIZE تم تعريفها في الخلية 4 (كانت 32)\n",
        "        steps_per_epoch = math.ceil(len(self.x_train) / BATCH_SIZE)\n",
        "\n",
        "        # 2. حلقة التدريب الخارجية (بايثون)\n",
        "        for epoch in range(LOCAL_EPOCHS_PER_ROUND):\n",
        "            print(f\"[Client {self.client_id}]: Starting local epoch {epoch+1}/{LOCAL_EPOCHS_PER_ROUND}...\")\n",
        "            train_flow = self.train_datagen.flow(\n",
        "                self.x_train, self.y_train, batch_size=BATCH_SIZE, shuffle=True, seed = 42\n",
        "            )\n",
        "\n",
        "            batch_count = 0\n",
        "            for x_batch, y_batch in train_flow:\n",
        "                # (حساب أوزان العينات)\n",
        "                sample_weights_tensor = None\n",
        "                if local_weight_dict:\n",
        "                    sample_weights_np = np.array([local_weight_dict[c] for c in y_batch])\n",
        "                    sample_weights_tensor = tf.convert_to_tensor(sample_weights_np, dtype=tf.float32)\n",
        "\n",
        "                loss_value = self._train_step(x_batch, y_batch, global_weights_tensors, mu, sample_weights_tensor)\n",
        "\n",
        "                if batch_count % 10 == 0:\n",
        "                    print(f\"[Client {self.client_id}]: Epoch {epoch+1}, Batch {batch_count}/{steps_per_epoch}, Loss: {loss_value.numpy():.4f}...\")\n",
        "\n",
        "                batch_count += 1\n",
        "\n",
        "                # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "                #  التعديل: إضافة شرط الخروج من الحلقة\n",
        "                # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "                if batch_count >= steps_per_epoch:\n",
        "                    break # <--- الخروج من الحلقة الداخلية\n",
        "\n",
        "            print(f\"[Client {self.client_id}]: === Finished local epoch {epoch+1}/{LOCAL_EPOCHS_PER_ROUND} ===\")\n",
        "\n",
        "        # تقييم الأداء\n",
        "        val_metrics = self.model.evaluate(self.x_val, self.y_val, verbose=0, return_dict=True)\n",
        "        print(f\"--- [Client {self.client_id}]: Finished fit(). Local val_acc: {val_metrics.get('accuracy', 0.0):.4f} ---\")\n",
        "        return self.model.get_weights(), len(self.x_train), {\"local_accuracy\": val_metrics.get('accuracy', 0.0)}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        self.model.set_weights(parameters)\n",
        "        metrics = self.model.evaluate(x_test_fl, y_test_fl, verbose=0, return_dict=True)\n",
        "        return metrics['loss'], len(x_test_fl), metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCBtkJsujKWz"
      },
      "source": [
        "---\n",
        "\n",
        "## الخطوة 10: تعريف استراتيجية الخادم (Server Strategy)\n",
        "\n",
        "سنقوم بتعريف دالة التقييم (`evaluate_on_server`) التي سيستخدمها الخادم لتقييم النموذج المُجمّع (Aggregated Model) في نهاية كل جولة باستخدام بيانات الاختبار العامة (`x_test_fl`).\n",
        "\n",
        "سنستخدم استراتيجية `FedAvg` (Federated Averaging) لتجميع الأوزان."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAtXucyLjLsp"
      },
      "outputs": [],
      "source": [
        "# --- 10. تعريف استراتيجيات الخادم ---\n",
        "from flwr.common import Context\n",
        "from flwr.server.strategy import FedAdam\n",
        "from flwr.common import ndarrays_to_parameters\n",
        "from flwr.common import NDArrays, Scalar\n",
        "from typing import Dict, Optional, Tuple\n",
        "\n",
        "\n",
        "# دالة تقييم على الخادم (محدثة لجمع كل المقاييس)\n",
        "def evaluate_on_server(server_round: int, parameters: NDArrays, config: Dict[str, Scalar]) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
        "    model = create_cnn_model()\n",
        "    model.set_weights(parameters)\n",
        "\n",
        "    metrics = model.evaluate(x_test_fl, y_test_fl, verbose=0, return_dict=True)\n",
        "\n",
        "    print(f\"\\n=======================================================\")\n",
        "    print(f\"ROUND {server_round}: Aggregated Accuracy = {metrics['accuracy']:.4f}\")\n",
        "    print(f\" (Loss: {metrics['loss']:.4f}, Prec: {metrics.get('precision', 0.0):.4f}, Rec: {metrics.get('recall', 0.0):.4f})\")\n",
        "    print(f\"=======================================================\\n\")\n",
        "\n",
        "    return metrics['loss'], metrics\n",
        "\n",
        "\n",
        "# دالة إنشاء العميل (باستخدام التعريف الأصلي cid: str)\n",
        "def client_fn(cid: str) -> XRayClient:\n",
        "    model = create_cnn_model()\n",
        "    client_x, client_y = client_data[int(cid)]\n",
        "    return XRayClient(model, client_x, client_y, client_id=cid)\n",
        "\n",
        "\n",
        "MIN_CLIENTS = int(NUM_CLIENTS * 0.8)\n",
        "\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "#  التعديل: تعريف الأوزان الأولية مرة واحدة للجميع\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "print(\"--- Creating initial model for ALL strategies ---\")\n",
        "model_for_init = create_cnn_model()\n",
        "initial_params = ndarrays_to_parameters(model_for_init.get_weights())\n",
        "print(\"--- Initial parameters created successfully ---\")\n",
        "\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "#  الإضافة: تقييم النموذج الأولي على بيانات التحقق\n",
        "#  (لنرسم نقطة البداية \"Epoch 0\" للنموذج المركزي)\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "print(\"--- Evaluating initial model on validation set (for Epoch 0 plot) ---\")\n",
        "# نستخدم 'validation_generator' لأن 'history' (المركزي) يستخدمها\n",
        "# نستخدم 'return_dict=True' لسهولة القراءة\n",
        "initial_model_val_metrics = model_for_init.evaluate(validation_generator, return_dict=True, verbose=0)\n",
        "print(f\"--- Initial val_accuracy (Epoch 0): {initial_model_val_metrics['accuracy']:.4f} ---\")\n",
        "\n",
        "# 1. استراتيجية FedAvg العادية (مدمجة)\n",
        "strategy_fedavg = fl.server.strategy.FedAvg(\n",
        "    initial_parameters=initial_params,\n",
        "    fraction_fit=1.0,\n",
        "    min_fit_clients=MIN_CLIENTS,\n",
        "    min_available_clients=NUM_CLIENTS,\n",
        "    evaluate_fn=evaluate_on_server,\n",
        "    fraction_evaluate=0.0\n",
        ")\n",
        "\n",
        "# 2. استراتيجية FedProx (المدمجة)\n",
        "strategy_fedprox = fl.server.strategy.FedProx(\n",
        "    initial_parameters=initial_params,\n",
        "    fraction_fit=1.0,\n",
        "    min_fit_clients=MIN_CLIENTS,\n",
        "    min_available_clients=NUM_CLIENTS,\n",
        "    evaluate_fn=evaluate_on_server,\n",
        "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    #  التعديل: زيادة \"قوة\" العقوبة 10x\n",
        "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    proximal_mu=PROXIMAL_MU,\n",
        "    fraction_evaluate=0.0\n",
        ")\n",
        "\n",
        "# 3. استراتيجية FedAdam (المدمجة)\n",
        "strategy_fedadam = FedAdam(\n",
        "    initial_parameters=initial_params,\n",
        "    fraction_fit=1.0,\n",
        "    min_fit_clients=MIN_CLIENTS,\n",
        "    min_available_clients=NUM_CLIENTS,\n",
        "    evaluate_fn=evaluate_on_server,\n",
        "    eta=SERVER_ETA,\n",
        "    beta_1=SERVER_BETA1,\n",
        "    beta_2=SERVER_BETA2,\n",
        "    tau=1e-7,\n",
        "    fraction_evaluate=0.0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHXx29h0jM5b"
      },
      "source": [
        "---\n",
        "\n",
        "## الخطوة 11: بدء محاكاة التعلم الاتحادي (FL Simulation)\n",
        "\n",
        "الآن، نبدأ المحاكاة لـ 15 جولة اتحادية (Federated Rounds)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SubqxGxUVydC"
      },
      "outputs": [],
      "source": [
        "# --- 11. بدء المحاكاة (لكل الاستراتيجيات) ---\n",
        "\n",
        "# NUM_ROUNDS defined in CONFIG cell above\n",
        "\n",
        "# إعدادات Ray لاستخدام الـ GPU\n",
        "ray_init_args = {\n",
        "    \"include_dashboard\": False,\n",
        "    \"num_gpus\": 1,\n",
        "    \"num_cpus\": 2    # <--- نضيف هذا لتحديد التوازي\n",
        "}\n",
        "\n",
        "# إخبار Flower بإعطاء 10% من الـ GPU لكل عميل\n",
        "client_resources = {\n",
        "    \"num_gpus\": 0.1,\n",
        "\n",
        "}\n",
        "\n",
        "print(f\"Ray init args: {ray_init_args}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYtSGRFBjN5C"
      },
      "outputs": [],
      "source": [
        "# 1. تشغيل FedAvg\n",
        "print(\"\\n--- بدء محاكاة (FedAvg) ---\")\n",
        "history_fedavg = fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS),\n",
        "    strategy=strategy_fedavg,\n",
        "    ray_init_args=ray_init_args,\n",
        "    client_resources=client_resources\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaidsY6HVvN2"
      },
      "outputs": [],
      "source": [
        "# 2. تشغيل FedProx\n",
        "print(\"\\n--- بدء محاكاة (FedProx) ---\")\n",
        "history_fedprox = fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS),\n",
        "    strategy=strategy_fedprox,\n",
        "    ray_init_args=ray_init_args,\n",
        "    client_resources=client_resources\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtvbvzQIVqSl"
      },
      "outputs": [],
      "source": [
        "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "#  إضافة جديدة: تشغيل FedAdam\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "print(\"\\n--- بدء محاكاة (FedAdam) ---\")\n",
        "history_fedadam = fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS),\n",
        "    strategy=strategy_fedadam,\n",
        "    ray_init_args=ray_init_args,\n",
        "    client_resources=client_resources\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AghRMffvjO9J"
      },
      "source": [
        "---\n",
        "\n",
        "## الخطوة 12: المقارنة النهائية\n",
        "\n",
        "أخيراً، نقوم بجمع النتائج من التدريب المركزي والتدريب الاتحادي ونعرضها في جدول ورسم بياني للمقارنة."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0oXn7BkizZd"
      },
      "outputs": [],
      "source": [
        "# --- 12. المقارنة النهائية (مقارنة 4 نماذج مع AUC) ---\n",
        "\n",
        "# (جلب مقاييس \"Epoch 0\" المركزية - يبقى كما هو)\n",
        "init_metrics = initial_model_val_metrics\n",
        "\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "#  التعديل: القراءة من القاموس الذي تم تحميله\n",
        "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "# بدلاً من history.history، نستخدم centralized_history_dict\n",
        "# (الذي تم تعريفه في الخلية 7، سواء بالتدريب أو بالتحميل)\n",
        "cen_epochs_hist = range(1, len(centralized_history_dict['val_accuracy']) + 1)\n",
        "cen_acc_hist = centralized_history_dict['val_accuracy']\n",
        "cen_loss_hist = centralized_history_dict['val_loss']\n",
        "cen_prec_hist = centralized_history_dict['val_precision']\n",
        "cen_rec_hist = centralized_history_dict['val_recall']\n",
        "cen_auc_hist = centralized_history_dict['val_auc']\n",
        "\n",
        "\n",
        "# (دمج \"Epoch 0\" مع باقي الدورات - يبقى كما هو)\n",
        "cen_epochs_plot = [0] + list(cen_epochs_hist)\n",
        "cen_acc_plot = [init_metrics['accuracy']] + cen_acc_hist\n",
        "cen_loss_plot = [init_metrics['loss']] + cen_loss_hist\n",
        "cen_prec_plot = [init_metrics.get('precision', 0.0)] + cen_prec_hist\n",
        "cen_rec_plot = [init_metrics.get('recall', 0.0)] + cen_rec_hist\n",
        "cen_auc_plot = [init_metrics.get('auc', 0.0)] + cen_auc_hist\n",
        "\n",
        "\n",
        "# (دالة استخراج المقاييس الاتحادية - تبقى كما هي)\n",
        "def extract_fl_history(fl_history, local_epochs_per_round):\n",
        "    loss_hist = fl_history.losses_centralized\n",
        "    metrics_hist = fl_history.metrics_centralized\n",
        "    x_axis = [(r * local_epochs_per_round) for r, _ in loss_hist]\n",
        "    loss = [l for r, l in loss_hist]\n",
        "    acc = [m for r, m in metrics_hist.get('accuracy', [])]\n",
        "    prec = [m for r, m in metrics_hist.get('precision', [])]\n",
        "    rec = [m for r, m in metrics_hist.get('recall', [])]\n",
        "    auc = [m for r, m in metrics_hist.get('auc', [])]\n",
        "    return x_axis, acc, loss, prec, rec, auc\n",
        "\n",
        "\n",
        "# (تحديد الدورات المحلية - يبقى كما هو)\n",
        "# LOCAL_EPOCHS_PER_ROUND = 3\n",
        "\n",
        "# (استخراج بيانات النماذج الاتحادية - يبقى كما هو)\n",
        "x_fedavg, acc_fedavg, loss_fedavg, prec_fedavg, rec_fedavg, auc_fedavg = extract_fl_history(history_fedavg, LOCAL_EPOCHS_PER_ROUND)\n",
        "x_fedprox, acc_fedprox, loss_fedprox, prec_fedprox, rec_fedprox, auc_fedprox = extract_fl_history(history_fedprox, LOCAL_EPOCHS_PER_ROUND)\n",
        "x_fedadam, acc_fedadam, loss_fedadam, prec_fedadam, rec_fedadam, auc_fedadam = extract_fl_history(history_fedadam, LOCAL_EPOCHS_PER_ROUND)\n",
        "\n",
        "\n",
        "# (كود الرسم البياني - يبقى كما هو تماماً)\n",
        "# ... (كل كود plt.subplot) ...\n",
        "plt.figure(figsize=(16, 18))\n",
        "\n",
        "# المخطط 1: الدقة (Accuracy)\n",
        "plt.subplot(3, 2, 1)\n",
        "plt.plot(cen_epochs_plot, cen_acc_plot, label='Centralized (Validation)', linestyle='--', color='red')\n",
        "plt.plot(x_fedavg, acc_fedavg, label='FedAvg', marker='o', color='blue', markersize=4)\n",
        "plt.plot(x_fedprox, acc_fedprox, label=f'FedProx (mu={PROXIMAL_MU})', marker='x', color='green', markersize=5)\n",
        "plt.plot(x_fedadam, acc_fedadam, label=f'FedAdam (eta={SERVER_ETA})', marker='s', color='purple', markersize=4)\n",
        "plt.title('Accuracy Progression')\n",
        "plt.xlabel('Equivalent Centralized Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# المخطط 2: الخسارة (Loss)\n",
        "plt.subplot(3, 2, 2)\n",
        "plt.plot(cen_epochs_plot, cen_loss_plot, label='Centralized (Validation)', linestyle='--', color='red')\n",
        "plt.plot(x_fedavg, loss_fedavg, label='FedAvg', marker='o', color='blue', markersize=4)\n",
        "plt.plot(x_fedprox, loss_fedprox, label=f'FedProx (mu={PROXIMAL_MU})', marker='x', color='green', markersize=5)\n",
        "plt.plot(x_fedadam, loss_fedadam, label=f'FedAdam (eta={SERVER_ETA})', marker='s', color='purple', markersize=4)\n",
        "plt.title('Loss Progression')\n",
        "plt.xlabel('Equivalent Centralized Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# المخطط 3: الدقة (Precision)\n",
        "plt.subplot(3, 2, 3)\n",
        "plt.plot(cen_epochs_plot, cen_prec_plot, label='Centralized (Validation)', linestyle='--', color='red')\n",
        "plt.plot(x_fedavg, prec_fedavg, label='FedAvg', marker='o', color='blue', markersize=4)\n",
        "plt.plot(x_fedprox, prec_fedprox, label=f'FedProx (mu={PROXIMAL_MU})', marker='x', color='green', markersize=5)\n",
        "plt.plot(x_fedadam, prec_fedadam, label=f'FedAdam (eta={SERVER_ETA})', marker='s', color='purple', markersize=4)\n",
        "plt.title('Precision Progression')\n",
        "plt.xlabel('Equivalent Centralized Epochs')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# المخطط 4: الاستدعاء (Recall)\n",
        "plt.subplot(3, 2, 4)\n",
        "plt.plot(cen_epochs_plot, cen_rec_plot, label='Centralized (Validation)', linestyle='--', color='red')\n",
        "plt.plot(x_fedavg, rec_fedavg, label='FedAvg', marker='o', color='blue', markersize=4)\n",
        "plt.plot(x_fedprox, rec_fedprox, label=f'FedProx (mu={PROXIMAL_MU})', marker='x', color='green', markersize=5)\n",
        "plt.plot(x_fedadam, rec_fedadam, label=f'FedAdam (eta={SERVER_ETA})', marker='s', color='purple', markersize=4)\n",
        "plt.title('Recall Progression')\n",
        "plt.xlabel('Equivalent Centralized Epochs')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# المخطط 5: AUC\n",
        "plt.subplot(3, 2, 5)\n",
        "plt.plot(cen_epochs_plot, cen_auc_plot, label='Centralized (Validation)', linestyle='--', color='red')\n",
        "plt.plot(x_fedavg, auc_fedavg, label='FedAvg', marker='o', color='blue', markersize=4)\n",
        "plt.plot(x_fedprox, auc_fedprox, label=f'FedProx (mu={PROXIMAL_MU})', marker='x', color='green', markersize=5)\n",
        "plt.plot(x_fedadam, auc_fedadam, label=f'FedAdam (eta={SERVER_ETA})', marker='s', color='purple', markersize=4)\n",
        "plt.title('AUC Progression')\n",
        "plt.xlabel('Equivalent Centralized Epochs')\n",
        "plt.ylabel('AUC')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEi9h-vrk9-O"
      },
      "outputs": [],
      "source": [
        "# --- 13. جدول ملخص النتائج النهائية ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"--- جدول ملخص النتائج النهائية (القيم النهائية من الجولة الأخيرة) ---\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# دالة آمنة لجلب القيمة الأخيرة من قائمة (لتجنب الأخطاء إذا كانت فارغة)\n",
        "def get_final_metric(metric_list):\n",
        "    try:\n",
        "        # إرجاع العنصر الأخير في القائمة\n",
        "        return metric_list[-1]\n",
        "    except (IndexError, TypeError):\n",
        "        # إرجاع 0.0 إذا كانت القائمة فارغة\n",
        "        return 0.0\n",
        "\n",
        "# 1. جلب بيانات النموذج المركزي\n",
        "# (من المتغير 'centralized_results' الذي خزنناه في الخلية 7)\n",
        "cen_metrics = {\n",
        "    'Accuracy': centralized_results.get('accuracy', 0.0),\n",
        "    'Loss': centralized_results.get('loss', 0.0),\n",
        "    'Precision': centralized_results.get('precision', 0.0),\n",
        "    'Recall': centralized_results.get('recall', 0.0),\n",
        "    'AUC': centralized_results.get('auc', 0.0)\n",
        "}\n",
        "\n",
        "# 2. جلب البيانات النهائية للنماذج الاتحادية\n",
        "# (من القوائم التي استخرجناها في الخلية 12)\n",
        "fedavg_metrics = {\n",
        "    'Accuracy': get_final_metric(acc_fedavg),\n",
        "    'Loss': get_final_metric(loss_fedavg),\n",
        "    'Precision': get_final_metric(prec_fedavg),\n",
        "    'Recall': get_final_metric(rec_fedavg),\n",
        "    'AUC': get_final_metric(auc_fedavg)\n",
        "}\n",
        "\n",
        "fedprox_metrics = {\n",
        "    'Accuracy': get_final_metric(acc_fedprox),\n",
        "    'Loss': get_final_metric(loss_fedprox),\n",
        "    'Precision': get_final_metric(prec_fedprox),\n",
        "    'Recall': get_final_metric(rec_fedprox),\n",
        "    'AUC': get_final_metric(auc_fedprox)\n",
        "}\n",
        "\n",
        "fedadam_metrics = {\n",
        "    'Accuracy': get_final_metric(acc_fedadam),\n",
        "    'Loss': get_final_metric(loss_fedadam),\n",
        "    'Precision': get_final_metric(prec_fedadam),\n",
        "    'Recall': get_final_metric(rec_fedadam),\n",
        "    'AUC': get_final_metric(auc_fedadam)\n",
        "}\n",
        "\n",
        "# 3. إنشاء قاموس البيانات لـ Pandas\n",
        "data = {\n",
        "    \"المقياس (Metric)\": [\"Accuracy\", \"Loss\", \"Precision\", \"Recall\", \"AUC\"],\n",
        "    \"المركزي (Centralized)\": [\n",
        "        cen_metrics['Accuracy'],\n",
        "        cen_metrics['Loss'],\n",
        "        cen_metrics['Precision'],\n",
        "        cen_metrics['Recall'],\n",
        "        cen_metrics['AUC']\n",
        "    ],\n",
        "    \"FedAvg\": [\n",
        "        fedavg_metrics['Accuracy'],\n",
        "        fedavg_metrics['Loss'],\n",
        "        fedavg_metrics['Precision'],\n",
        "        fedavg_metrics['Recall'],\n",
        "        fedavg_metrics['AUC']\n",
        "    ],\n",
        "    f\"FedProx (mu={PROXIMAL_MU})\": [\n",
        "        fedprox_metrics['Accuracy'],\n",
        "        fedprox_metrics['Loss'],\n",
        "        fedprox_metrics['Precision'],\n",
        "        fedprox_metrics['Recall'],\n",
        "        fedprox_metrics['AUC']\n",
        "    ],\n",
        "    f\"FedAdam (eta={SERVER_ETA})\": [\n",
        "        fedadam_metrics['Accuracy'],\n",
        "        fedadam_metrics['Loss'],\n",
        "        fedadam_metrics['Precision'],\n",
        "        fedadam_metrics['Recall'],\n",
        "        fedadam_metrics['AUC']\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 4. إنشاء وطباعة الجدول\n",
        "summary_df = pd.DataFrame(data)\n",
        "\n",
        "# نستخدم .to_string() لضمان طباعة الجدول بالكامل بتنسيق نظيف\n",
        "print(summary_df.to_string(index=False, float_format='%.4f'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}